# Task-Aware Dynamic Model Optimization for Multi-Task Learning
ğŸ“„ IEEE Access 2023 â€” [DOI: 10.1109/ACCESS.2023.3339793](https://doi.org/10.1109/ACCESS.2023.3339793)

This repository introduces the core ideas and implementation of  
**Task-Aware Dynamic Model Optimization (DMO)**, a memory-efficient multi-task learning (MTL) framework.

---

## ğŸ” Overview

Multi-task learning (MTL) often suffers from **task interference** and **inefficient resource usage**.  
**DMO (Dynamic Model Optimization)** addresses these issues by:

- Grouping tasks based on **inter-task similarity** (weight & loss correlations)
- Dynamically allocating parameters based on **task difficulty**
- Producing **lightweight subnetworks**, reducing parameters by over **80%** on average

<p align="center">
  <img src="https://raw.githubusercontent.com/sujin-1013/Task-Aware-DMO/main/assets/dmo_overview.png" width="700"/>
</p>
<sub><i>Figure: Overview of Task-Aware Dynamic Model Optimization (DMO)</i></sub>

---

## ğŸ¯ Key Components

- **Task Similarity Measurement**
  - Pearson correlation on weights (`S_W`)
  - Pearson correlation on losses (`S_L`)
- **Task Grouping**
  - Graph-based maximal clique detection (Bronâ€“Kerbosch)
- **Dynamic Parameter Allocation**
  - Pruning by task difficulty
  - Merging into group-specific subnetworks

---

## ğŸ› ï¸ Code Structure

```
Task-Aware-DMO/
â”œâ”€â”€ models/           # Backbone network, task-specific heads, pruning utilities
â”œâ”€â”€ train.py          # Training logic
â”œâ”€â”€ evaluate.py       # Evaluation and metrics
â”œâ”€â”€ config/           # Experimental settings
â”œâ”€â”€ results/          # Graphs and tables from the paper
â”œâ”€â”€ assets/           # Images and visual materials for README
â””â”€â”€ paper/            # Original PDF paper for reference
```


---

## ğŸ§ª Datasets Used

- CIFAR-10, STL-10, MNIST, USPS
- CIFAR-100 (20-task version)
- Visual Decathlon Challenge (10 diverse tasks)

---

## ğŸ“Š Result Summary

| Dataset Group         | Param Reduction | Accuracy (Avg)   |
|----------------------|-----------------|------------------|
| CIFAR-10 + MNIST     | ~84%            | â­¤ Comparable     |
| CIFAR-100 (20 Tasks) | ~85%            | â¬†ï¸ Improved       |
| Decathlon Challenge  | ~88%            | â­¤ Slight Drop     |

> ğŸ”¹ Up to **97% reduction** on USPS with minimal performance loss

---

### ğŸ§ª Visual Decathlon Challenge (Summary Table)

| Method                   | Avg Accuracy (%) | # Parameters (M) | Param Ratio |
|--------------------------|------------------|------------------|-------------|
| Single-task              | **74.69**        | 101.68           | **1.00**    |
| Soft Param Sharing [7]   | 65.24            | 101.68           | 1.00        |
| Cross-stitch [8]         | 76.33            | 203.36           | 2.00        |
| NDDR [10]                | 59.76            | 252.86           | 2.49        |
| TAPS [6]                 | 59.11            | 130.50           | 1.28        |
| **DMO (Ours)**           | 74.40            | **11.48**        | **0.11**    |

---

### ğŸ“‘ Visual Decathlon Table (From Paper)

<p align="center">
  <img src="https://raw.githubusercontent.com/sujin-1013/Task-Aware-DMO/main/assets/visual_decathlon_table.png" width="700"/>
</p>
<sub><i>Figure: Visual Decathlon Challenge results table (from IEEE Access paper)</i></sub>

---

## ğŸ§‘â€ğŸ’» Author

**Su-jin Choi (ìµœìˆ˜ì§„)**  
Master of AI, Chung-Ang University  
ğŸ“§ popo2419@naver.com  
ğŸ“„ [IEEE Access Paper](https://doi.org/10.1109/ACCESS.2023.3339793)
